{
    "train": true,
    "model_name": "meta-llama/Llama-3.1-8B",
    "token_name": "meta-llama/Llama-3.1-8B-Instruct",
    "enable_fsdp": false,
    "low_cpu_fsdp": false,
    "run_validation": true,
    "batch_size_training": 16,
    "num_epochs": 1,
    "num_workers_dataloader": 1,
    "weight_decay": 0.0,
    "gamma": 0.85,
    "seed": 2,
    "use_fp16": false,
    "mixed_precision": true,
    "val_batch_size": 1,
    "micro_batch_size": 16,
    "model_use_peft": false,
    "output_dir": "ckpt",
    "freeze_layers": false,
    "num_freeze_layers": 1,
    "quantization": false,
    "one_gpu": false,
    "save_model": true,
    "dist_checkpoint_root_folder": "./checkpoints",
    "dist_checkpoint_folder": "llama3",
    "ret_checkpoint_folder": "",
    "save_optimizer": false,
    "use_fast_kernels": false,
    "peft_method": "None",
    "lr": 2e-5,
    "memory_bank_length": 0,
    "dataset": "llava_llama3_selfrag_multi_dataset",
    "single": false,
    "all_gather": false,
    "add_vocab": false,
    "natural_form": false,
    "target_modules": "q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj"
}
